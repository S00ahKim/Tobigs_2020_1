## Matrix Review 
**차원**? 
공간 내의 있는 점 등의 위치를 나타내기 위해 필요한 축의 개수 = 독립변수의 개수

**차원의 저주**? 
변수가 늘어나고, 차원이 커지면서 발생하는 문제

    1. 필요한 데이터의 수 지수함수적 증식

    2. 정보의 밀도 감소
    (같은 개수의 데이터라도 차원이 다르면 차지하는 밀도가 달라짐)
    
    3. 공간을 설명하기 위한 데이터의 부족 -> 과적합 문제 (특정 영역의 데이터만 있을 수 있어서??)
    특정 차원까지는 성능 증가하다가 감소
    

    -> 차원축소가 필요한 이유!

**차원축소**?
현재의 독립개수보다 더 적은 독립변수로 데이터를 설명해보자 (3차원->2차원)
-> 차원 축소로 차원의 저주 해결, 연산량 감소, 정보밀도 증가, 시각화 용이(3차원 이하일 경우)

**차원의 축소 방법?**
    1. 변수선택(feature selection) : 불필요한 변수 제거 
    ex. Lasso
    Lasso에서 정규화의 정도 클수록 beta가 0에 근접 -> 변수선택 효과 (cf. Ridge)
    2. 변수추출(feature extraction) : 원본 데이터의 조합으로 새로운 특징 생성 (기존의 데이터를 대체) 
    ex. PCA, LDA, T-SNE

차원의 저주를 해결하기 위해 PCA, LDA, T-SNE 등의 변수추출을 이용해 차원축소를 해보자!


1. Vector : 
변수의 나열

2. eigen vector(영벡터 제외), eigen value (square matrix를 대상으로)
n*n matrix는 복소수범위에서 eigen value n개를 갖는다. 

3. basis(기저) : 
벡터공간을 표현할 수 있는 최소한의 벡터 집합
(=벡터공간을 span + 벡터끼리 linearly independent)
+) standard basis
n차 단위행렬의 열벡터는 R^n의 기저이며, 이를 standard basis라고 한다.

4. 기저 변환
벡터공간의 basis는 unique한 것이 아니다.
벡터공간을 표현할 수 있는 최소한의 단위인 기저를 변경

5. 고유벡터의 정규화 
(=고유벡터의 크기를 1으로 만들자)
하나의 eigen value에 대응되는 eigen vector는 어떤 벡터의 span이다. (즉 여러 개의 벡터가 고유벡터)
하지만 우리는 하나의 eigen value에 하나의 eigen vector가 대응되기를 원한다.
따라서 이 중에 크기가 1인 벡터를 이 eigen value의 eigen vector로 보는 것이다.

**eigen decomposition**(고유값 분해; 고유값을 구하자!)
det(A-rambda*I)=0 인 rambda를 구하고, 이 고유값에 대응되는 eigen vector를 구한다.

**spectral decomposition**
n차 대칭행렬 A는 n개의 eigen value과 n개의 normal eigen vector를 갖는다.
 
(여기서 e1, e2, … en은 정규화된 eigen vector)
여기서 eigen vector가 new 기저가 된다.
A와 rambda1*e1*e1.T의 차이가 적을수록 좋은 것!
PCA는 공분산행렬을 spectral decomposition해서 고유값이 큰 고유벡터를 찾아내는 것!

Q. 왜 큰 고유값을 찾아야 하는가?
 
A. 고유값이 클수록 그 고유벡터 방향이 원본 데이터를 많이 설명해주는 것
(여기서 0.9가 빨강의 고유값, 0.1이 노랑의 고유값)

---
## PCA

**주성분분석**?
변수들의 전체분산 대부분을 소수의 주성분을 통하여 설명하는 것
고차원 데이터의 최대 분산 방향을 찾아 새로운 공간에 저차원으로 투영하는 것
기하학적 측면에서 좌표축을 회전시켜 얻어진 새로운 좌표축 선택
 


**공분산행렬**
대각원소 k번째 원소는 k번째 변수의 분산
k행 i열의 원소는 k번째 변수와 i번째 변수의 공분산
-> 공분산행렬은 대칭행렬(symmetric matrix)

PCA에서는 이 공분산행렬을 분해한다. (공분산행렬은 symm square matrix)
(대칭행렬에서 고유벡터는 직교한다.)
고유벡터 : 데이터가 분산된 방향

데이터 투영 -> projection

공분산 행렬의 모든 고유벡터는 직교한다. 즉, 모든 주성분은 독립이다. 
(공분산행렬이 대칭행렬이라서 성립)

**주성분 개수의 결정**
    1. elbow point (y축 eigen value : eigen value를 내림차순 정렬한 후 plot)
    2. kaiser’s rule (고유값 1 이상의 주성분들만 선택)
    3. 누적설명률(= 선택한 고유값의 합/전체 고유값의 합)이 70%~80% 이상인 지점
    * 고유값은 기존 데이터를 잘 설명하는 정도라고 보면 된다.

 

 
고유값과 고유벡터의 linear combination으로 데이터가 설명되므로 데이터가 선형성을 띄어야 한다.
변수 간 상관관계를 고려(공분산행렬을 분해함) -> 공선성 부분적 해결
주성분끼리는 선형독립이기 때문에 다중공선성 문제 해결
값의 scale이 크면 고유값도 큰 경향이 있기 때문에 값의 scale이 작은 변수가 상대적으로 무시되는 효과
따라서 PCA에서 scaling 필수 (단위의 영향력을 제거하기 위함)

 

그냥 데이터의 변동을 잘 설명할 수 있는 축으로 변경하자는 것이지,
이 변경한 데이터가 원본 데이터보다 target을 좀 더 정확하게 맞추는 방향으로 이뤄진다는 보장 X
(위에서 2차원의 데이터를 1차원으로 축소했더니 이전보다 빨강/파랑 구별이 어려워졌다 ☹)
주성분에 대한 해석이 어렵다는 단점

---

##LDA(선형 판별 분석)
PCA는 라벨을 보지 않고 압축된 데이터를 생성 (압축된 데이터가 기존데이터를 잘 설명할 수 있는 방향으로)
LDA는 **라벨을 고려**하여 압축된 데이터를 생성
(LDA는 binary 뿐만 아니라 multiclass를 고려하는 것도 가능하다)

PCA도, LDA도 데이터가 선형데이터일 때만 성능 좋다 😊

고유값이 별로 차이가 없다면 -> 하나의 고유벡터의 축으로 압축해도 성능 좋지 않을 수 있다.


PCA, LDA는 데이터가 선형적일 때 성능 좋다
데이터가 둘둘 말려있는 형태라면?

거리의 개념이 다르다 
다양체를 manifold라고 부른다.

Isomap : 
특정 지점을 근접해서 보면 선형적으로 보인다.
LLE
국소적인 평면의 데이터들이 축소된 차원에서도 인접하도록 반영
SNE : 거리를 확률모델을 이용해
t 분포를 잡으면 t-SNE가 된다.

비선형적 데이터를 PCA, LDA 압축하면 압축하기 전보다 성능 떨어진다.



